{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Winery classification using the one-dimensional Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Wine** data set is the running example for our discussion of the *generative approach to classification*. \n",
    "\n",
    "The data can be downloaded from the UCI repository (https://archive.ics.uci.edu/ml/datasets/wine). It contains 178 labeled data points, each corresponding to a bottle of wine:\n",
    "* The features (`x`): a 13-dimensional vector consisting of visual and chemical features for the bottle of wine\n",
    "* The label (`y`): the winery from which the bottle came (1,2,3)\n",
    "\n",
    "Before continuing, download the data set and place it in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load in the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the packages we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: ok\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: ok\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension --sys-prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard includes\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Useful module for dealing with the Gaussian density\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "# installing packages for interactive graphs\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the Wine data set. There are 178 data points, each with 13 features and a label (1,2,3).\n",
    "We will divide these into a training set of 130 points and a test set of 48 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'wine.data.txt' needs to be in the same directory\n",
    "data = np.loadtxt('wine.data.txt', delimiter=',')\n",
    "# Names of features\n",
    "featurenames = ['Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash','Magnesium', 'Total phenols', \n",
    "                'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', \n",
    "                'OD280/OD315 of diluted wines', 'Proline']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix a particular \"random\" permutation of the data, and use these to effect the training / test split.\n",
    "We get four arrays:\n",
    "* `trainx`: 130x13, the training points\n",
    "* `trainy`: 130x1, labels of the training points\n",
    "* `testx`: 48x13, the test points\n",
    "* `testy`: 48x1, labels of the test points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 178 instances into training set (trainx, trainy) of size 130 and test set (testx, testy) of size 48\n",
    "# Also split apart data and labels\n",
    "np.random.seed(0)\n",
    "perm = np.random.permutation(178)\n",
    "trainx = data[perm[0:130],1:14]\n",
    "trainy = data[perm[0:130],0]\n",
    "testx = data[perm[130:178], 1:14]\n",
    "testy = data[perm[130:178],0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many training points there are from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43, 54, 33)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(trainy==1), sum(trainy==2), sum(trainy==3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Fast exercise</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you figure out how many test points there are from each class? *Note down these three numbers: you will enter it as part of this week's programming assignment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 17, 15)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modify this cell\n",
    "sum(testy==1), sum(testy==2), sum(testy==3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Look at the distribution of a single feature from one of the wineries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick just one feature: 'Alcohol'. This is the first feature, that is, number 0. Here is a *histogram* of this feature's values under class 1, along with the *Gaussian fit* to this distribution.\n",
    "\n",
    "<img src=\"histogram.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm: how can we generate a figure like this? \n",
    "\n",
    "The following function, **density_plot**, does this for any feature and label. The first line adds an interactive component that lets you choose these parameters using sliders. \n",
    "\n",
    "<font color=\"magenta\">Try it out!</font> And then, look at the code carefully to understand exactly what it is doing, line by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655af0ff7ca94c00ab8746debc872291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='feature', max=12), IntSlider(value=1, description='label…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual( feature=IntSlider(0,0,12), label=IntSlider(1,1,3))\n",
    "def density_plot(feature, label):\n",
    "    plt.hist(trainx[trainy==label,feature], density=True)\n",
    "    #\n",
    "    mu = np.mean(trainx[trainy==label,feature]) # mean\n",
    "    var = np.var(trainx[trainy==label,feature]) # variance\n",
    "    std = np.sqrt(var) # standard deviation\n",
    "    #\n",
    "    x_axis = np.linspace(mu - 3*std, mu + 3*std, 1000)\n",
    "    plt.plot(x_axis, norm.pdf(x_axis,mu,std), 'r', lw=2)\n",
    "    plt.title(\"Winery \"+str(label) )\n",
    "    plt.xlabel(featurenames[feature], fontsize=14, color='red')\n",
    "    plt.ylabel('Density', fontsize=14, color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Fast exercise</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function **density_plot**, the code for plotting the Gaussian density focuses on the region within 3 standard deviations of the mean. Do you see where this happens? Why do you think we make this choice?\n",
    "\n",
    "Here's something for you to figure out: for which feature (0-12) does the distribution of (training set) values for winery 1 have the *smallest* standard deviation? Write down the answer: you will need to enter it as part of this week's programming assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.82962509e-01, 6.56756786e-01, 1.91767278e-01, 2.45766535e+00,\n",
       "       1.08840191e+01, 3.43734147e-01, 3.90396479e-01, 5.96428889e-02,\n",
       "       4.53274368e-01, 1.22463376e+00, 1.15433202e-01, 3.55846328e-01,\n",
       "       2.20103973e+02])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modify this cell\n",
    "std = np.zeros(13)\n",
    "for feature in range(0,13):\n",
    "    std[feature] = np.std(trainx[trainy==1,feature])\n",
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05964288894974403\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(std.min())\n",
    "std.argmin()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fit a Gaussian to each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that will fit a Gaussian generative model to the three classes, restricted to just a single feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes y takes on values 1,2,3\n",
    "def fit_generative_model(x,y,feature):\n",
    "    k = 3 # number of classes\n",
    "    mu = np.zeros(k+1) # list of means\n",
    "    var = np.zeros(k+1) # list of variances\n",
    "    pi = np.zeros(k+1) # list of class weights\n",
    "    for label in range(1,k+1):\n",
    "        indices = (y==label)\n",
    "        mu[label] = np.mean(x[indices,feature])\n",
    "        var[label] = np.var(x[indices,feature])\n",
    "        pi[label] = float(sum(indices))/float(len(y))\n",
    "    return mu, var, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call this function on the feature 'alcohol'. What are the class weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33076923 0.41538462 0.25384615]\n"
     ]
    }
   ],
   "source": [
    "feature = 0 # 'alcohol'\n",
    "mu, var, pi = fit_generative_model(trainx, trainy, feature)\n",
    "print(pi[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, display the Gaussian distribution for each of the three classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3058ee06934b8bb187ce4dec4e3f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='feature', max=12), Button(description='Run Interact', st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual( feature=IntSlider(0,0,12) )\n",
    "def show_densities(feature):\n",
    "    mu, var, pi = fit_generative_model(trainx, trainy, feature)\n",
    "    colors = ['r', 'k', 'g']\n",
    "    for label in range(1,4):\n",
    "        m = mu[label]\n",
    "        s = np.sqrt(var[label])\n",
    "        x_axis = np.linspace(m - 3*s, m+3*s, 1000)\n",
    "        plt.plot(x_axis, norm.pdf(x_axis,m,s), colors[label-1], label=\"class \" + str(label))\n",
    "    plt.xlabel(featurenames[feature], fontsize=14, color='red')\n",
    "    plt.xlim(0,25)\n",
    "    plt.ylabel('Density', fontsize=14, color='red')\n",
    "    # plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Fast exercise</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the widget above to look at the three class densities for each of the 13 features. Here are some questions for you:\n",
    "* For which feature (0-12) do the densities for classes 1 and 3 *overlap* the most?\n",
    "* For which feature (0-12) is class 3 the most spread out relative to the other two classes?\n",
    "* For which feature (0-12) do the three classes seem the most *separated* (this is somewhat subjective at present)?\n",
    "\n",
    "*Write down the answers to these questions: you will enter them as part of this week's assignment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature 0: [0.         0.23325279 0.2819047  0.2851787 ]\n",
      "feature 1: [0.         0.43132948 1.17309945 1.36014105]\n",
      "feature 2: [0.         0.03677469 0.11230233 0.02675978]\n",
      "feature 3: [ 0.          6.04011898 10.79627229  4.3902663 ]\n",
      "feature 4: [  0.         118.46187128 328.50925926 125.42332415]\n",
      "feature 5: [0.         0.11815316 0.3214298  0.14217539]\n",
      "feature 6: [0.         0.15240941 0.56869729 0.07375317]\n",
      "feature 7: [0.         0.00355727 0.01546735 0.01345748]\n",
      "feature 8: [0.         0.20545765 0.40247932 0.2018371 ]\n",
      "feature 9: [0.         1.49972785 0.90598999 5.89504246]\n",
      "feature 10: [0.         0.01332482 0.0366261  0.01305253]\n",
      "feature 11: [0.         0.12662661 0.27074074 0.06275831]\n",
      "feature 12: [    0.         48445.75878853 25189.68484225 15330.99173554]\n"
     ]
    }
   ],
   "source": [
    "for feature in range(13):\n",
    "    mu, var, pi = fit_generative_model(trainx, trainy, feature)\n",
    "    print(f'feature {feature}: {var}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In feature 9, class 3 is the most spread out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature 0: [ 0.         13.78534884 12.31092593 13.15969697]\n",
      "feature 0: [0.         0.23325279 0.2819047  0.2851787 ]\n",
      "\n",
      "feature 1: [0.         2.02232558 1.91925926 3.37727273]\n",
      "feature 1: [0.         0.43132948 1.17309945 1.36014105]\n",
      "\n",
      "feature 2: [0.         2.42790698 2.22703704 2.40090909]\n",
      "feature 2: [0.         0.03677469 0.11230233 0.02675978]\n",
      "\n",
      "feature 3: [ 0.         16.88139535 19.67592593 20.89393939]\n",
      "feature 3: [ 0.          6.04011898 10.79627229  4.3902663 ]\n",
      "\n",
      "feature 4: [  0.         105.8372093   95.83333333  99.03030303]\n",
      "feature 4: [  0.         118.46187128 328.50925926 125.42332415]\n",
      "\n",
      "feature 5: [0.         2.85162791 2.3012963  1.67606061]\n",
      "feature 5: [0.         0.11815316 0.3214298  0.14217539]\n",
      "\n",
      "feature 6: [0.         2.99627907 2.10907407 0.75727273]\n",
      "feature 6: [0.         0.15240941 0.56869729 0.07375317]\n",
      "\n",
      "feature 7: [0.         0.28906977 0.34740741 0.45969697]\n",
      "feature 7: [0.         0.00355727 0.01546735 0.01345748]\n",
      "\n",
      "feature 8: [0.         1.93069767 1.65722222 1.20848485]\n",
      "feature 8: [0.         0.20545765 0.40247932 0.2018371 ]\n",
      "\n",
      "feature 9: [0.         5.63023256 3.1837037  7.38242421]\n",
      "feature 9: [0.         1.49972785 0.90598999 5.89504246]\n",
      "\n",
      "feature 10: [0.         1.06232558 1.0412963  0.68666667]\n",
      "feature 10: [0.         0.01332482 0.0366261  0.01305253]\n",
      "\n",
      "feature 11: [0.         3.16674419 2.76666667 1.67484848]\n",
      "feature 11: [0.         0.12662661 0.27074074 0.06275831]\n",
      "\n",
      "feature 12: [   0.         1141.90697674  525.98148148  635.90909091]\n",
      "feature 12: [    0.         48445.75878853 25189.68484225 15330.99173554]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for feature in range(13):\n",
    "    mu, var, pi = fit_generative_model(trainx, trainy, feature)\n",
    "    print(f'feature {feature}: {mu}')\n",
    "    print(f'feature {feature}: {var}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict labels for the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well can we predict the class (1,2,3) based just on one feature? The code below lets us find this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea042971d40e4966bd493a1c29aab06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='feature', max=12), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact( feature=IntSlider(0,0,12) )\n",
    "def test_model(feature):\n",
    "    mu, var, pi = fit_generative_model(trainx, trainy, feature)\n",
    "\n",
    "    k = 3 # Labels 1,2,...,k\n",
    "    n_test = len(testy) # Number of test points\n",
    "    score = np.zeros((n_test,k+1))\n",
    "    for i in range(0,n_test):\n",
    "        for label in range(1,k+1):\n",
    "            score[i,label] = np.log(pi[label]) + \\\n",
    "            norm.logpdf(testx[i,feature], mu[label], np.sqrt(var[label]))\n",
    "    predictions = np.argmax(score[:,1:4], axis=1) + 1\n",
    "    # Finally, tally up score\n",
    "    errors = np.sum(predictions != testy)\n",
    "    print(\"Test error using feature \" + featurenames[feature] + \": \" + str(errors) + \"/\" + str(n_test)+ f' {round(errors/n_test*100,2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">One last exercise</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are looking at classifiers that use just one out of a possible 13 features. Choosing a subset of features is called **feature selection**. In general, this is something we would need to do based solely on the *training set*--that is, without peeking at the *test set*.\n",
    "\n",
    "For the wine data, compute the training error and test error associated with each choice of feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your findings, answer the following questions:\n",
    "* Which three features have the lowest training error? List them in order (best first).\n",
    "* Which three features have the lowest test error? List them in order (best first).\n",
    "\n",
    "*Note down your answers: you will enter them later, as part of this week's programming assignment*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_errors(mode):\n",
    "    err_pct_dict, err_pct_lst = {}, []\n",
    "    if mode == \"train\":\n",
    "        x, y = trainx, trainy\n",
    "    elif mode == \"test\":\n",
    "        x,y = testx, testy\n",
    "    for feature in range(13):\n",
    "        mu, var, pi = fit_generative_model(trainx, trainy, feature)\n",
    "    \n",
    "        k = 3 # Labels 1,2,...,k\n",
    "        n_elem = len(y) # Number of test points\n",
    "        score = np.zeros((n_elem,k+1))\n",
    "        for i in range(0,n_elem):\n",
    "            for label in range(1,k+1):\n",
    "                score[i,label] = np.log(pi[label]) + \\\n",
    "                norm.logpdf(x[i,feature], mu[label], np.sqrt(var[label]))\n",
    "        predictions = np.argmax(score[:,1:4], axis=1) + 1\n",
    "        # Finally, tally up score\n",
    "        errors = np.sum(predictions != y)\n",
    "        errors = round(errors/n_elem*100)\n",
    "        err_pct_lst.append(errors)\n",
    "        err_pct_dict[feature]  = errors\n",
    "    return err_pct_dict, err_pct_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 34, 1: 38, 2: 51, 3: 52, 4: 47, 5: 35, 6: 21, 7: 42, 8: 46, 9: 29, 10: 37, 11: 36, 12: 27}\n",
      "[21, 27, 29, 34, 35, 36, 37, 38, 42, 46, 47, 51, 52]\n"
     ]
    }
   ],
   "source": [
    "train_error_dict, train_error_lst = model_errors('train')\n",
    "print(train_error_dict)\n",
    "print(sorted(train_error_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 35, 1: 48, 2: 60, 3: 48, 4: 44, 5: 33, 6: 17, 7: 48, 8: 33, 9: 21, 10: 29, 11: 40, 12: 35}\n",
      "[17, 21, 29, 33, 33, 35, 35, 40, 44, 48, 48, 48, 60]\n"
     ]
    }
   ],
   "source": [
    "test_error_dict, test_error_lst = model_errors('test')\n",
    "print(test_error_dict)\n",
    "print(sorted(test_error_lst))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_DS",
   "language": "python",
   "name": "ml_ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
